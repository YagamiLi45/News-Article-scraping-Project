{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url='https://indianexpress.com/section/sports/'\n",
    "\n",
    "#it is for sport \n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def scrape_news_articles(url, csv_filename):\n",
    "    # Send an HTTP request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the HTML elements containing the links to the news articles\n",
    "        article_links = soup.find_all('div', class_='img-context')\n",
    "\n",
    "        # Create a CSV file and write header\n",
    "        with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = ['Title', 'URL', 'Content']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            # Iterate through the links and extract information from each article\n",
    "            for link in article_links:\n",
    "                # Example: Use a hypothetical getchild function to extract child elements\n",
    "                child_elements = getchild(link, 'h2', class_='title')\n",
    "\n",
    "                # Check if the 'h2' tag with class 'title' is found\n",
    "                if child_elements:\n",
    "                    # Extracting the URL from the 'href' attribute of the 'a' tag within 'h2'\n",
    "                    article_url = child_elements.find('a').get('href', '')\n",
    "\n",
    "                    # Extracting the text content of the 'h2' tag\n",
    "                    article_title = child_elements.text.strip()\n",
    "                    # print(article_title)\n",
    "\n",
    "                    # Fetch the content of each individual article\n",
    "                    # article_content = fetch_article_content(article_url, 'p', class_='img-context')\n",
    "                    # Write the data to the CSV file\n",
    "                \n",
    "                \n",
    "                child_elements_p = getchild(link, 'p').text.strip()\n",
    "                # print(child_elements_p)\n",
    "                writer.writerow({'Title': article_title, 'URL': article_url, 'Content': child_elements_p})\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the page. Status Code: {response.status_code}\")\n",
    "\n",
    "# Rest of your code ...\n",
    "\n",
    "# Make sure to define the getchild and fetch_article_content functions as well\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getchild(element, tag_name, **kwargs):\n",
    "    # A hypothetical function to get child elements based on tag name and attributes\n",
    "    return element.find(tag_name, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we collecting the articles from the newspaper dataset\n",
    "# scrape_news_articles('https://indianexpress.com/section/sports/', 'sports_articles.csv')\n",
    "# scrape_news_articles('https://indianexpress.com/section/political-pulse/', 'politics_articles.csv')\n",
    "# scrape_news_articles('https://indianexpress.com/section/education/', 'education_articles.csv')\n",
    "# scrape_news_articles('https://indianexpress.com/section/lifestyle/', 'lifestyle_articles.csv')\n",
    "# scrape_news_articles('https://indianexpress.com/section/business/', 'business_articles.csv')\n",
    "# scrape_news_articles('https://indianexpress.com/section/opinion/', 'opinion_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opinion article\n",
    "scrape_news_articles('https://indianexpress.com/section/opinion/', 'opinion_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_column_to_csv(csv_name, new_column_name, new_column_value):\n",
    "    # Read the existing CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_name)\n",
    "\n",
    "    # Assign the same value for all rows in the new column\n",
    "    df[new_column_name] = new_column_value\n",
    "\n",
    "    # Write the updated DataFrame back to the existing CSV file\n",
    "    df.to_csv(csv_name, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add columns for each csv file which has category / We are labeling the data \n",
    "add_column_to_csv('sports_articles.csv','Category' ,'Sports')\n",
    "add_column_to_csv('business_articles.csv','Category' ,'Business')\n",
    "add_column_to_csv('education_articles.csv','Category' ,'Education')\n",
    "add_column_to_csv('lifestyle_articles.csv','Category' ,'Lifestyle')\n",
    "add_column_to_csv('opinion_articles.csv','Category' ,'Opinion')\n",
    "add_column_to_csv('politics_articles.csv','Category' ,'Polictics')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_csv_files(csv_files, output_csv):\n",
    "    # Create an empty DataFrame to hold the concatenated data\n",
    "    concatenated_df = pd.DataFrame()\n",
    "\n",
    "    # Iterate over each CSV file and concatenate the DataFrames\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        concatenated_df = pd.concat([concatenated_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "    # Write the concatenated DataFrame to a new CSV file\n",
    "    concatenated_df.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"CSV files concatenated successfully. Result saved to: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we concatenate all the files \n",
    "\n",
    "# csv_files_all=['business_articles.csv','education_articles.csv','lifestyle_articles.csv','opinion_articles.csv','politics_articles.csv','sports_articles.csv']\n",
    "# output_csvfile='news_articles_dataset.csv'\n",
    "# concatenate_csv_files(csv_files_all,output_csvfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping part is done and also we have the dataset with different categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\shant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\shant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\shant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "nltk.download('wordnet') #\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#############Tokenizer###############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##################Stopwords##################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##################Lemmatizer#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'alpha': 0.1}\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.88\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Business       1.00      1.00      1.00         6\n",
      "   Education       0.88      1.00      0.93         7\n",
      "   Lifestyle       0.67      1.00      0.80         2\n",
      "   Polictics       1.00      0.67      0.80         6\n",
      "      Sports       0.75      0.75      0.75         4\n",
      "\n",
      "    accuracy                           0.88        25\n",
      "   macro avg       0.86      0.88      0.86        25\n",
      "weighted avg       0.90      0.88      0.88        25\n",
      "\n",
      "Model and Vectorizer pickled successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "\n",
    "# Sample data loading (replace this with your actual data loading code)\n",
    "df = pd.read_csv('news_articles_dataset.csv')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Content'], df['Category'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Custom Tokenizer class\n",
    "class MyTokenizer:\n",
    "    def transform(self, X):\n",
    "        return [' '.join(doc) if isinstance(doc, list) else doc for doc in X]\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Tokenization and TF-IDF Vectorization for training data\n",
    "X_train_processed = MyTokenizer().transform(X_train)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_processed)\n",
    "\n",
    "# Tokenization and TF-IDF Vectorization for testing data\n",
    "X_test_processed = MyTokenizer().transform(X_test)\n",
    "X_test_tfidf = vectorizer.transform(X_test_processed)\n",
    "\n",
    "# Fine-tune hyperparameters using GridSearchCV\n",
    "param_grid = {\n",
    "    'alpha': [0.01, 0.1, 1.0, 10.0]  # Adjust this range based on your problem\n",
    "}\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "grid_search = GridSearchCV(classifier, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = best_classifier.predict(X_train_tfidf)\n",
    "y_pred_test = best_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Training Accuracy:\", accuracy_train)\n",
    "print(\"Testing Accuracy:\", accuracy_test)\n",
    "\n",
    "# Classification report for testing data\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "\n",
    "# Pickle the best model and the vectorizer for later use\n",
    "with open('best_classifier.pkl', 'wb') as model_file:\n",
    "    pickle.dump(best_classifier, model_file)\n",
    "\n",
    "with open('vectorizer.pkl', 'wb') as vectorizer_file:\n",
    "    pickle.dump(vectorizer, vectorizer_file)\n",
    "\n",
    "print(\"Model and Vectorizer pickled successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Due to less data the training accuracy is 100 % and testing accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
